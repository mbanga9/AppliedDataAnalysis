{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from datetime import datetime\n",
    "from article_selection import article_selection\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are used to handle the data and extract the wanted articles from the \n",
    "'.xml'files and store them into an array of strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Takes a start and an end date and retuns a list of all the months in between of the form mm/yyyy.\n",
    "def month_dates(start, end):\n",
    "    f = lambda date: date.month + 12 * date.year\n",
    "\n",
    "    res = []\n",
    "    for tot_m in range(f(start)-1, f(end)):\n",
    "        y, m = divmod(tot_m, 12)\n",
    "        res.append(str(y) + '/' + '%02d' % (m+1))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Takes an article as input and output the date of this articles in the format dd/mm/yyy. \n",
    "def get_date(article):\n",
    "    \"\"\"\n",
    "    This method returns the date of the article\n",
    "    \"\"\"\n",
    "    str_date = article.find('entity').find('meta').find('issue_date').text\n",
    "    return datetime.strptime(str_date, '%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get all the articles in a '.xml' file and store the into an array of strings.\n",
    "def get_articles_in_file(file, start_date, end_date):\n",
    "    articles = []  \n",
    "    for article in file.iter('article'):\n",
    "        if article.find('entity') is not None:\n",
    "            a = ''\n",
    "            date = get_date(article)\n",
    "            if start_date <= date <= end_date:\n",
    "                for entity in article.iter('entity'):\n",
    "                    a += entity.findtext('full_text') + ' '\n",
    "                articles.append(date.strftime('%d/%m/%Y') + ' ' + a)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using 'get_articles_in_file' goes throught all the '.xml' files in 'path' to store them in an array of strings.\n",
    "def get_articles(path, start_date, end_date):\n",
    "    articles = []\n",
    "    for m_date in month_dates(start_date, end_date):\n",
    "        try:\n",
    "            file = etree.parse(path + m_date + '.xml')\n",
    "            articles.append(get_articles_in_file(file, start_date, end_date))\n",
    "        except (FileNotFoundError, IOError):\n",
    "            pass\n",
    "    return [a for file in articles for a in file]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Takes as input a box_id and an '.xml' file and returns the text corresponding to box_id the the '.xml' file.\n",
    "def get_entity_text(file, box_id):\n",
    "    res = None\n",
    "    for article in file.iter('article'):\n",
    "        if article.find('entity') is not None:\n",
    "            date = get_date(article)\n",
    "            for entity in article.iter('entity'):\n",
    "                if   box_id == entity.find('meta').find('box').text:\n",
    "                    res = date.strftime('%d/%m/%Y') + ' ' + entity.findtext('full_text')\n",
    "                    break\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/mbanga/Desktop/JDG/'\n",
    "start_date =  datetime(1990, 1, 1)\n",
    "end_date = datetime(1998, 2, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles = get_articles(path, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fr_core_news_sm\n",
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = fr_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizations of the articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we got our articles we decided to lemmatize them before processing a classification algorithm on it so we could get better results.These are the functions we use for the lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to eliminate tokens that are pure punctuaiton or whitespace.\n",
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuaiton or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to eliminate tokens that are not french words.\n",
    "def is_french(word):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens that\n",
    "    are not french words.\n",
    "    \"\"\"\n",
    "    d = enchant.Dict('fr_FR')\n",
    "    return d.check(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generator function to use spaCy to parse articles,lemmatize the text, and yield sentences.\n",
    "def lemmatized_corpus(corpus):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse articles,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "\n",
    "    pos = ['VERB', 'PROPN', 'NOUN', 'ADJ', 'ADV']\n",
    "    for parsed_article in nlp.pipe(corpus, \n",
    "                                   batch_size=100, n_threads=5):\n",
    "        # save the date\n",
    "        date = parsed_article[0].text\n",
    "        \n",
    "        yield (date, ' '.join([token.lemma_ for token in parsed_article if token.pos_ in pos]))\n",
    "                             \n",
    "        '''if not punct_space(token) and is_french(token.text)\n",
    "                                and not token.is_stop and not token.is_digit\n",
    "                                and not token.like_num]))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaÃ¯ve Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the fact that we a huge dataset of articles, We dicide to at first filter the articles using a simple selection by keywords.We initialize an array of string that are related to the tematic of the 'Votation', We might be losing some articles that would be meaningfull but regarding the size of our dataset we are ready to make this concession.We also think that it has some sence to do a keywords selection because it would hard to have an article about 'Votations' that does not cointains any word of our keywords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This fonction take an array of articles and a list of keywords 'lemmas' and returns all the articles that coitains one of more the words in our keywords list\n",
    "def corpus_votation(articles, lemmas):\n",
    "    votations = []\n",
    "    for article in articles:\n",
    "        if any(lemma in article for lemma in lemmas): \n",
    "            votations.append(article)\n",
    "    return votations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus_votation_bis(articles, lemmas):\n",
    "    votations = []\n",
    "    for article in articles:\n",
    "        if any(lemma in article.replace(' ', '') for lemma in lemmas):\n",
    "            votations.append(article)\n",
    "    return votations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive selection (First Filtering)\n",
    "lems = ['votation', 'referendum']\n",
    "\n",
    "articles_votation = article_selection(articles, lems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(articles_votation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lemmatization of the corpus of aticles selected using our naive selection and storing it in tuple of (dates,articles)\n",
    "if 0 == 1:\n",
    "    %%time\n",
    "    # Time consuming !!\n",
    "    lemmatized_corpus = [(date, lemmas) for date, lemmas in lemmatized_corpus(articles_votation)]\n",
    "\n",
    "    # retrieve dates\n",
    "    dates = [pair[0] for pair in lemmatized_corpus]\n",
    "\n",
    "    # retrieve articles\n",
    "    corpus = [pair[1] for pair in lemmatized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Storing the articles we lemmatized before in '.txt' file.\n",
    "if 0 == 1:\n",
    "    project_path = '/home/mbanga/Epfl/AppliedDataAnalysis/ADA2017_GroupWork/Project/'\n",
    "\n",
    "    with open(os.path.join(project_path, 'lemmatized articles 1990-1998.txt'), 'w') as file:\n",
    "        for article in corpus:\n",
    "            file.write(article + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Storing the articles we lemmatized before in '.json' file.\n",
    "if 0 == 1:\n",
    "    project_path = '/home/mbanga/Epfl/AppliedDataAnalysis/ADA2017_GroupWork/Project/'\n",
    "\n",
    "    with open(os.path.join(project_path, 'lemmatized articles 1990-1998 json'), 'w') as file:\n",
    "        json.dump(lemmatized_corpus, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    # check ouput of lemmatizer (lemmatized_corpus) \n",
    "    file = etree.parse('/home/mbanga/Desktop/JDG/1990/01.xml')\n",
    "    box_id = '24 123 1446 2167'\n",
    "\n",
    "    original_text = [get_entity_text(file, box_id)]\n",
    "\n",
    "    for lemmatized in lemmatized_corpus(original_text):\n",
    "        print(lemmatized[1], '\\n')\n",
    "    print(original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    # check naive selection\n",
    "    file = etree.parse('/home/mbanga/Desktop/JDG/1990/01.xml')\n",
    "    box_id = '50 163 1090 888'\n",
    "\n",
    "    original_text = [get_entity_text(file, box_id)]\n",
    "    lemmas = ['vote', 'voter', 'votation', 'referendum']\n",
    "    res = corpus_votation(original_text, lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering articles about votations\n",
    "\n",
    "> Assumption: The subject of a votation is most likely to be found in\n",
    "the neighborhoud of the terms 'votation' or 'referendum' in the article. \n",
    "So we decided to extract the sentecence that cointais the keywords along with the sentences before and after.We consider that a sentence begins and end with a ',' which is usually the case but since the dataset that we have is not perfectly clean some errors occur collecting sentecens that are not really complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all phrases index with the searched term\n",
    "keywords = ['votation']\n",
    "#j = 0\n",
    "\n",
    "articles_votation_sents = []\n",
    "for article in articles_votation:\n",
    "    date = re.findall(r'^([^\\s]+)', article)[0]\n",
    "    #print('article', (j+1), ': ', date)\n",
    "    \n",
    "    sent = ''\n",
    "    phrases = article.split('.')    \n",
    "    for i, phrase in enumerate(phrases):\n",
    "        if any(keyword in phrase for keyword in keywords):\n",
    "            if len(phrases) < 2:\n",
    "                sent += phrase\n",
    "            elif i == 0:\n",
    "                sent += phrase[phrase.index(' ') + 1:] + ' '  + phrases[i+1]\n",
    "            elif i == len(phrases) - 1:\n",
    "                sent += ' ' + phrases[i-1] + ' ' + phrase\n",
    "            elif 0 < i < len(phrases) - 1:\n",
    "                sent += ' ' + phrases[i-1] + ' ' + phrase + ' ' + phrases[i+1]\n",
    "    articles_votation_sents.append(date + ' ' + sent)\n",
    "            #print(' {:}'.format(phrases[i-1] + phrase + phrases[i+1]))\n",
    "    #print('\\n')\n",
    "    #j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(articles_votation_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles_votation_sents[501]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1 == 1:\n",
    "    %%time\n",
    "    # Time consuming !!\n",
    "    lemmatized_corpus = [(date, lemmas) for date, lemmas in lemmatized_corpus(articles_votation_sents)]\n",
    "\n",
    "    # retrieve dates\n",
    "    dates = [pair[0] for pair in lemmatized_corpus]\n",
    "\n",
    "    # retrieve articles\n",
    "    corpus = [pair[1] for pair in lemmatized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(lemmatized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatized_corpus[501]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the articles that we got in our dataset are in french is was quite difficult to find a training dataset to fit a model that be able to classify our articles.We decide to use the Latent dirichlet allocation as our natural languge processing tool.Our aim was to minimize the bais of our topic classfication of the articles we exctracted.We could assign the mainstream votation topics(i.e army,economy,education...) and try to extract statics regarding a well defined set,but we did not want to make these kind of assumptions about the existance or the importance of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "#import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learn the dictionnary by iterating over all of the articles\n",
    "dico = Dictionary([article.split() for article in corpus])\n",
    "\n",
    "# filter tokens that are very rare or too common from\n",
    "# the dictionary \n",
    "dico.filter_extremes(no_below=0, no_above=0.4)\n",
    "\n",
    "# reassign integer lda\n",
    "dico.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generator function to read articles from a file and yield a bag-of-words representation.  \n",
    "def bow_generator(corpus):\n",
    "    \"\"\"\n",
    "    generator function to read articles from a file\n",
    "    and yield a bag-of-words representation\n",
    "    \"\"\"\n",
    "    for article in corpus:\n",
    "        yield dico.doc2bow(article.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate bag-of-word representations for\n",
    "# all reviews and save them as a matrix\n",
    "project_path = '/home/mbanga/Epfl/AppliedDataAnalysis/ADA2017_GroupWork/Project/'\n",
    "MmCorpus.serialize(os.path.join(project_path, 'corpus.mm'),\n",
    "                                bow_generator(corpus))\n",
    "\n",
    "bow_corpus = MmCorpus(os.path.join(project_path, 'corpus.mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# storing our model\n",
    "lda_model_filepath = os.path.join(project_path, 'lda_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1 == 1:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "\n",
    "        # workers => sets the parallelism, and should be\n",
    "        # set to your number of physical cores minus one\n",
    "        lda = LdaMulticore(bow_corpus,\n",
    "                           num_topics=50,\n",
    "                           id2word=dico,\n",
    "                           workers=5)\n",
    "        \n",
    "        lda.save(lda_model_filepath)\n",
    "\n",
    "#load the finished LDA model from disk\n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accept a topic number and print out a formatted list of the top terms.\n",
    "def explore_topic(topic_number, topn=25):\n",
    "    \"\"\"\n",
    "    accept a user-supplied topic number and\n",
    "    print out a formatted list of the top terms\n",
    "    \"\"\"\n",
    "    \n",
    "    print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "    \n",
    "    for term, frequency in lda.show_topic(topic_number, topn):\n",
    "        print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "explore_topic(topic_number=15, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The goal is to find all documents related to the same topic\n",
    "def articles_topic(lda, bow_corpus, corpus, topic):\n",
    "    \"\"\"\n",
    "    return the list of articles associated\n",
    "    with a given topic.\n",
    "    \"\"\"\n",
    "    assert len(bow_corpus) == len(corpus)\n",
    "    nb_topics = len(lda.get_topics())\n",
    "    \n",
    "    documents = []\n",
    "    if 0 <= topic < nb_topics:\n",
    "        k = 0\n",
    "        for bow_article in bow_corpus:\n",
    "            dist = lda.get_document_topics(bow_article, minimum_probability=0)\n",
    "            dist = [p[1] for p in dist]\n",
    "            idx_max = dist.index(max(dist))\n",
    "            if idx_max == topic:\n",
    "                documents.append(corpus[k])\n",
    "            k += 1\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = articles_topic(lda, bow_corpus, articles_votation_sents, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1 == 1:     \n",
    "        LDAvis_prepared = pyLDAvis.gensim.prepare(lda, bow_corpus, dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
