{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do we really have the freedom to vote on what we want to ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scope of this project our focus lies on written political articles in newspapers.\n",
    "We are willing to assess the diversity of subjects submitted for votations to Swiss \n",
    "residents over the last 200 years. We essentially want to classify political articles\n",
    "in order to identify trends, distributions, densities or patterns among others over the decades.\n",
    "\n",
    "Therefore, we seek to analyse \"Le Temps digital archives and data\". This dataset consists of\n",
    "articles representing two centuries of informations provided by no more existing newspapers,\n",
    "namely 'Le journal de Gèneve' and 'La Gazette de Lausanne'. These are ancestors of today\n",
    "well-known Swiss newspaper 'Le Temps' whose publications are written in the French language.\n",
    "\n",
    "Available through well structured xml files one can retrieves information from any given period\n",
    "of time. Each xml file gathers indeed articles published during a specific month of a year.\n",
    "Specifically the dataset consist of 4'335 xml files. The period of time covered by those file\n",
    "ranges from February 1798 to February 1998.\n",
    "\n",
    "It is important to specify here that we are focussing on highlevel subjects. The aim is of being\n",
    "able to identity subjects such as 'army', 'Health & care' or 'Educations' among others.\n",
    "We do not ambition to discriminate between fine grained subjects contained in those just\n",
    "mentionned. As an example, subjects such as 'compulsory school' and 'university' will not\n",
    "be distinguishable from one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "import enchant\n",
    "import pandas as pd\n",
    "import fr_core_news_sm\n",
    "\n",
    "from lxml import etree\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "#import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_retrieval import *\n",
    "from data_reduction import *\n",
    "from data_cleaning import *\n",
    "from lda_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/mbanga/Documents/EPFL/ADA/'\n",
    "start_date =  datetime(1990, 1, 1)\n",
    "end_date = datetime(1990, 1, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Time consuming\n",
    "articles_path = os.path.join(path, 'JDG/')\n",
    "if 1 == 1:\n",
    "    articles = get_articles(articles_path, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We deploy preprocessing strategies to improve the quality of the results of the unsupervised\n",
    "classification algorithm we are using. These strategies have been chosen based on our understanding\n",
    "of the data at hand and the outcome we focus on. \n",
    "\n",
    "In order to discriminate between articles related to 'votations' from  others we take\n",
    "a very simple yet sensitive approach. We argue that is it very unlikely that any publication\n",
    "related to swiss 'votations' does not contain the word 'votation' or 'referendum'. Of course,\n",
    "we can discuss about the accuracy of such a method. We may certainly generate false positive\n",
    "or discard relevant articles. Yet the sample size we obtain with this first process appears to\n",
    "be large enough to capture the information we are interested in.\n",
    "\n",
    "TODO: Creates cells showing this assertion\n",
    "(As an example, the described reduction brings down the sample to ~3000 articles for the year 1990.)\n",
    "\n",
    "Second, we made another assumption that reduce the length of any given publication related to\n",
    "'votations'. The main motive behind this process is based on the results obtained considering\n",
    "articles entirely. As mentionned earlier the goal is to identify and classify highlevel\n",
    "publications about 'votations'. That information is usually contained in a single word whose\n",
    "position in the article is usually close to the keyword 'votation'. As a consequence we retain\n",
    "only sentences containing the keyword 'votation' and its closest neighboors, namely the \n",
    "preciding and following sentences. Again we are aware that this filtering method may discard\n",
    "relevant information. More importantly, this assumption may reveal disastruous for our results\n",
    "if it appears to be erroneous. \n",
    "\n",
    "TODO: Provide with these examples\n",
    "To convince the reader that it is indeed a satisfying assumption, we provide a list of (20) randomly\n",
    "(you can trust :-)) selected articles we applied the described filtering on. One can see that\n",
    "in most of the cases the subject can be found close to the keyword.\n",
    "\n",
    "\n",
    "Besides of the two filtering techniques we have described so far, more traditional data \n",
    "preprocessing techniques have been employed in the project. We describe them hereafter:\n",
    "\n",
    "TODO: Describe the other techniques (lemmatization, remove stopwords, ect)\n",
    "\n",
    "TODO: save and show results when we do not delete some dimensions to argue on our process to retrieve information\n",
    "Before using our model to define topics related to Swiss votations over the past two centuries we have\n",
    "to employ prepocessing strategies. This preprocessing step in mandatory whenever we want to come up with\n",
    "meaningful results. Here we summarize techniques that have been used in the project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the fact that we a huge dataset of articles, We dicide to at first filter the articles using a simple selection by keywords.We initialize an array of string that are related to the tematic of the 'Votation', We might be losing some articles that would be meaningfull but regarding the size of our dataset we are ready to make this concession.We also think that it has some sence to do a keywords selection because it would hard to have an article about 'Votations' that does not cointains any word of our keywords list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Assumption: The subject of a votation is most likely to be found in\n",
    "the neighborhoud of the terms 'votation' or 'referendum' in the article. \n",
    "So we decided to extract the sentecence that cointais the keywords along with the sentences before and after.We consider that a sentence begins and end with a ',' which is usually the case but since the dataset that we have is not perfectly clean some errors occur collecting sentecens that are not really complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forget this idea as we don't have spacy on the cluster and can't install additional packages\n",
    "\n",
    "# todo: add an autocorrect, to rectify spelling mistakes in parsed text\n",
    "# most interesting is a spacy add-on for hunspell (very new):\n",
    "# https://github.com/tokestermw/spacy_hunspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defines keywords that should be contained in articles\n",
    "# to consider them votations\n",
    "# keywords = ['votation']\n",
    "# todo: check if notebook file .ipynb encoded in UTF\n",
    "keywords = ['votation','voter','référendum',' élection','Élection','initiative populaire', \n",
    "            # careful with 'élection': includes all articles with sélection\n",
    "            # adding a space fixes this: ' élection'\n",
    "            'grand conseil','plébiscite','scrutin','suffrage']\n",
    "# todo: add removing of keywords from articles\n",
    "# get articles related to votations\n",
    "original_corpus = filter_articles(articles, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(original_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_corpus[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize articles about votations\n",
    "corpus = summarize_articles(original_corpus, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Time consuming !!\n",
    "\n",
    "# For each publication ee keep only words that occupy one of\n",
    "# the listed grammatical positions in the sentence\n",
    "pos=['VERB', 'PROPN', 'NOUN', 'ADJ', 'ADV']\n",
    "if 1 == 1:\n",
    "    %%time\n",
    "    cleaned = [(date, lemmas) for date, lemmas in clean(corpus, pos)]\n",
    "\n",
    "    # retrieve dates\n",
    "    dates = [pair[0] for pair in cleaned]\n",
    "\n",
    "    # retrieve articles\n",
    "    corpus = [pair[1] for pair in cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_path = '/home/mbanga/Documents/EPFL/ADA/Project_NLP/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Storing the articles we lemmatized before in '.txt' file.\n",
    "if 0 == 1:\n",
    "\n",
    "    with open(os.path.join(project_path, 'cleanedCorpus1990-1998.txt'), 'w') as file:\n",
    "        for article in corpus:\n",
    "            file.write(article + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Storing the articles we lemmatized before in '.json' file.\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(os.path.join(project_path, 'cleanedCorpus1990-1998.json'), 'w') as file:\n",
    "        json.dump(lemmatized_corpus, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading articles from .json file\n",
    "with open(os.path.join(project_path, 'cleanedCorpus1990-1998.json'), 'r') as file:\n",
    "    lemmatized_corpus = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    # check ouput of cleaner\n",
    "    file = etree.parse(os.path.join(path, 'JDG/1990/01.xml'))\n",
    "    box_id = '24 123 1446 2167'\n",
    "\n",
    "    original_text = [get_entity_text(file, box_id)]\n",
    "\n",
    "    for lemmatized in clean(original_text, pos):\n",
    "        print(lemmatized[1], '\\n')\n",
    "    print(original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    # check naive selection\n",
    "    file = etree.parse(os.path.join(path, 'JDG/1990/01.xml'))\n",
    "    box_id = '50 163 1090 888'\n",
    "\n",
    "    original_text = [get_entity_text(file, box_id)]\n",
    "    lemmas = ['vote', 'voter', 'votation', 'referendum']\n",
    "    res = summarize_articles(original_text, keywords=lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the articles that we got in our dataset are in french is was quite difficult to find a training dataset to fit a model that be able to classify our articles.We decide to use the Latent dirichlet allocation as our natural languge processing tool.Our aim was to minimize the bais of our topic classfication of the articles we exctracted.We could assign the mainstream votation topics(i.e army,economy,education...) and try to extract statics regarding a well defined set,but we did not want to make these kind of assumptions about the existance or the importance of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learn the dictionnary by iterating over all of the articles\n",
    "dico = Dictionary([article.split() for article in corpus])\n",
    "\n",
    "# filter tokens that are very rare or too common from\n",
    "# the dictionary \n",
    "dico.filter_extremes(no_below=0, no_above=0.4)\n",
    "\n",
    "# reassign integer lda\n",
    "dico.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate bag-of-word representations for\n",
    "# all reviews and save them as a matrix\n",
    "project_path = '/home/mbanga/Documents/EPFL/ADA/Project_NLP/'\n",
    "\n",
    "if 1 == 1:\n",
    "    MmCorpus.serialize(os.path.join(project_path, 'corpus.mm'),\n",
    "                       bow_generator(corpus, dico))\n",
    "    \n",
    "\n",
    "bow_corpus = MmCorpus(os.path.join(project_path, 'corpus.mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# storing our model\n",
    "lda_model_filepath = os.path.join(project_path, 'lda_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1 == 1:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "\n",
    "        # workers => sets the parallelism, and should be\n",
    "        # set to your number of physical cores minus one\n",
    "        lda = LdaMulticore(bow_corpus,\n",
    "                           num_topics=5,\n",
    "                           id2word=dico,\n",
    "                           workers=1)\n",
    "        \n",
    "        lda.save(lda_model_filepath)\n",
    "\n",
    "# load the finished LDA model from disk\n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "explore_topic(lda, topic_number=3, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_docs = articles_from_topic(lda, bow_corpus, original_corpus, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(topic_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1 == 1:     \n",
    "        LDAvis_prepared = pyLDAvis.gensim.prepare(lda, bow_corpus, dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(bow_corpus[10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
